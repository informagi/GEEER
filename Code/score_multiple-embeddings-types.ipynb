{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d377a50",
   "metadata": {},
   "source": [
    "For ease of running multiple experiments, the code has been extracted out of the python script and put into a jupyter notebook\n",
    "\n",
    "Below are all settins for running experiments, please uncomment the four arguments corresponding with a certain experiment.\n",
    "\n",
    "Make sure to run all cells. The last two cells will output the commandline scripts to run the ranklib script, and then run the evaluation afterwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c4cd1-0e06-4083-ae6d-b7c0a9f7f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class args(object):\n",
    "    # The following lines correspond to: Wikipedia2Vec 2019:\n",
    "    # embedding = '../src/WKN-vectors/WKN-vectors.bin'\n",
    "    # ranklib_output = '../w2vconcepts'\n",
    "    # score_output = '../w2vconcepts.txt'\n",
    "    # embedding_type = 'wikipedia2vec_old'\n",
    "\n",
    "    # The following lines correspond to: Wikipedia2Vec 2015:\n",
    "    # embedding = '../src/wikipedia2vec_trained_20151002/wikipedia-20151002_trained_500' \n",
    "    # ranklib_output = '../w2v2015concepts'\n",
    "    # score_output = 'w2v2015concepts.txt'\n",
    "    # embedding_type = 'wikipedia2vec'\n",
    "    \n",
    "    #The following lines correspond to: rdf2vec\n",
    "    # embedding = '../src/walks_1306/model.kv'\n",
    "    # ranklib_output = '../rdf2vec1306concepts'\n",
    "    # score_output = '../rdf2vec1306concepts.txt'\n",
    "    # embedding_type = \"rdf2vec\"\n",
    "\n",
    "    # The following lines correspond to: rdf2vec + pagelinks\n",
    "    # embedding = '../src/walks_1708/model.kv'\n",
    "    # ranklib_output = '../rdf2vec1708concepts'\n",
    "    # score_output = '../rdf2vec1708concepts.txt'\n",
    "    # embedding_type = \"rdf2vec\"\n",
    "\n",
    "    # The following lines correspond to: complex\n",
    "    embedding = '../src/20220511-231012-dbpedia_small-complex_gpu/checkpoint_best.pt'\n",
    "    ranklib_output = '../complex2vecredconcepts'\n",
    "    score_output = '../complex2vecredconcepts.txt'\n",
    "    embedding_type = 'complex'\n",
    "    \n",
    "\n",
    "    # The following lines correspond to: complex + pagelinks\n",
    "    # embedding = '../src/20220818-151937-dbpedia_pagelinks-complex_gpu/checkpoint_best.pt'\n",
    "    # ranklib_output ='../complex2vecpagelinksconcepts'\n",
    "    # score_output = '../complex2vecpagelinksconcepts.txt'\n",
    "    # embedding_type = 'complex'\n",
    "    \n",
    "    \n",
    "    # Use any of the following linkers, or other linkers in the /Data directory: \n",
    "\n",
    "    #linker = \"../Data/geeer_ready.csv\"\n",
    "    #linker = \"../Data/geeer_annotated_radboud.csv\"\n",
    "    linker = \"../Data/geeer_annotated_ELQ.csv\"\n",
    "    #linker = \"../Data/geeer_annotated_total.csv\"\n",
    "\n",
    "    dbpedia_input = \"../src/DBpedia-Entity/runs/v2/bm25f-ca_v2.run\"\n",
    "    def __init__(self, i_var):\n",
    "        self.i_var = i_var\n",
    "\n",
    "path_to_dbpedia = \"../src/DBpedia-Entity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de842836-cf8c-4969-bc2e-1469ad67665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.embedding):\n",
    "    print(\"Embedding file does not exist! Make sure you have unzipped the file in /src\")\n",
    "else:\n",
    "    print(\"Found Embeddings\")\n",
    "if not os.path.exists(args.dbpedia_input):\n",
    "    print(\"Dbpedia file does not exist! Make sure you have unzipped the file in /src\")\n",
    "else:\n",
    "    print(\"Found Dbpedia\")\n",
    "if not os.path.exists(args.linker):\n",
    "    print(\"Linker file does not exist!\")\n",
    "else:\n",
    "    print(\"Found Linker\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80790104-960a-43b5-b91e-5fd5cdf4c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_lookup(tag, model, tagme = False):\n",
    "    # Input: entity string\n",
    "    # Output: True wikipedia2vec format\n",
    "    # Looking up the embedding of entities, returning [] when entity not in corpus\n",
    "    if tagme and args.embedding_type == 'rdf2vec':\n",
    "        tag = tag.replace('ENTITY/', 'http://dbpedia.org/resource/')\n",
    "    elif tagme and args.embedding_type == 'complex':\n",
    "        tag = tag.replace('ENTITY/', '<http://dbpedia.org/resource/')\n",
    "        tag = tag + '>'\n",
    "    elif tagme and args.embedding_type == 'wikipedia2vec':\n",
    "        tag = tag.replace('ENTITY/', '')\n",
    "        tag = tag.replace('_', ' ')\n",
    "    elif tagme and args.embedding_type == 'wikipedia2vec_old':\n",
    "        tag = tag\n",
    "    elif tag in redirect_dict:\n",
    "        tag = redirect_dict[tag]\n",
    "    else:\n",
    "        try: \n",
    "            backup = entity_converter(redirect_dict[tag])\n",
    "        except KeyError as e:\n",
    "            backup = ''\n",
    "        tag = entity_converter(tag)\n",
    "    if args.embedding_type == 'wikipedia2vec': \n",
    "        if model.get_entity(tag):\n",
    "            return model.get_entity_vector(tag)\n",
    "        else:\n",
    "            return []\n",
    "    if args.embedding_type == 'rdf2vec' or args.embedding_type == 'wikipedia2vec_old':\n",
    "        #if tag in model.vocab:\n",
    "        if tag in model.key_to_index:\n",
    "            return model[tag]\n",
    "        else:\n",
    "            return []\n",
    "    if args.embedding_type == 'complex': \n",
    "        #if tag in model.dataset.entity_ids():\n",
    "        if tag in entity_ids_dict:\n",
    "            #ent_index = model.dataset.entity_ids().index(tag)\n",
    "            ent_index = entity_ids_dict[tag]\n",
    "            model_tag = model.get_s_embedder().embed(torch.Tensor([ent_index]).long())\n",
    "            return model_tag\n",
    "        elif backup in entity_ids_dict:\n",
    "            ent_index = entity_ids_dict[backup]\n",
    "            model_tag = model.get_s_embedder().embed(torch.Tensor([ent_index]).long())\n",
    "            #print(tag, backup)\n",
    "\n",
    "            return model_tag\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e175f-deb9-4c3c-9a8c-eccadbf15352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_converter(word, reverse = False, nospace = True):\n",
    "    # Input: an entity string in dbpedia format\n",
    "    # Output: string in wikipedia2vec format or readible format\n",
    "    # Getting entities in the right format\n",
    "    if args.embedding_type == 'rdf2vec':\n",
    "        word = word.replace(\"<dbpedia:\", \"http://dbpedia.org/resource/\")\n",
    "        word = word.replace(\">\", \"\")\n",
    "        return(word)\n",
    "    elif args.embedding_type == 'complex':\n",
    "        word = word.replace(\"dbpedia:\", \"http://dbpedia.org/resource/\")\n",
    "        return(word)\n",
    "    elif args.embedding_type == 'wikipedia2vec':\n",
    "        word = word.replace(\"<dbpedia:\", \"\")\n",
    "        word = word.replace(\">\", \"\")\n",
    "        word = word.replace(\"_\", \" \")\n",
    "        return(word)\n",
    "    elif args.embedding_type == 'wikipedia2vec_old':\n",
    "        word = word.replace(\"<dbpedia:\", \"ENTITY/\")\n",
    "        word = word.replace(\">\", \"\")\n",
    "        return word\n",
    "    elif reverse:\n",
    "        word = word.replace(\"<dbpedia:\", \"\")\n",
    "        word = word.replace(\">\", \"\")\n",
    "        if nospace:\n",
    "            return word\n",
    "        else:\n",
    "            word = word.replace(\"_\", \" \")\n",
    "            return word\n",
    "    else:\n",
    "        word = word.replace(\"<dbpedia:\", \"ENTITY/\")\n",
    "        word = word.replace(\">\", \"\")\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efab727-2e1d-4186-98a4-af5087e1705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding_type == 'complex':\n",
    "    import torch\n",
    "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "def ranking_feature(query, entity, dist = \"cosine\", conf = True):\n",
    "    # Input: a query-entity pair to score, a distance function and if we want to use confidence scores\n",
    "    # Output: query-entity based score\n",
    "\n",
    "    # getting all the linked entities to the query\n",
    "    #a_ent = pd.DataFrame(confidence.loc[confidence['query_id'] == query])\n",
    "    if query not in confidence:\n",
    "    #if len(a_ent)== 0:\n",
    "        # if no linked queries, return 0\n",
    "        return 0\n",
    "    else:\n",
    "        total = 0\n",
    "        a_ent = confidence[query]\n",
    "        for row in a_ent:\n",
    "        #for index, row in a_ent.iterrows():\n",
    "            score = row['confidence']\n",
    "            #score = row['score']\n",
    "            ent1 = entity_lookup(entity, model)\n",
    "            ent2 = entity_lookup(row['tag'], model, tagme = True)\n",
    "            #ent2 = entity_lookup(row['entity'])\n",
    "            if len(ent1) == 0 or len(ent2) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if dist == 'fjaccard':\n",
    "                    dist = 1-np.minimum(ent1,ent2).sum()/np.maximum(ent1,ent2).sum()\n",
    "                elif args.embedding_type == 'complex':\n",
    "                    dist = cos(ent1, ent2).detach().numpy()[0]\n",
    "                else:\n",
    "                    dist = 1 - spatial.distance.cosine(ent1, ent2)\n",
    "                if conf:\n",
    "                    score = row['confidence']\n",
    "                    total += score*dist\n",
    "                else:\n",
    "                    total += dist\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e607f4-cec7-456f-8efe-f4be9f3149c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_print_format(queries, filepath):\n",
    "    # input: A list of queries of which we want the results\n",
    "    # output: Ranklib ready format of results\n",
    "    f = open(filepath, \"w\")\n",
    "    for q in queries:\n",
    "        entities = new_df.loc[(new_df['query_id'] == q)]\n",
    "        for index, row in entities.iterrows():\n",
    "            printstring = str(int(row['rel'])) + ' qid:' + row['query_id'] + \" 1:\" + str(row['embedding_score']) + \" 2:\" + str(row['fsdm_score']) + \" # \" + row['tag']\n",
    "            print(printstring, file = f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8db255-4924-42d9-bdf8-b09d4758a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_print_format_dict(queries, filepath):\n",
    "    # input: A list of queries of which we want the results\n",
    "    # output: Ranklib ready format of results\n",
    "    f = open(filepath, \"w\")\n",
    "    for q in queries:\n",
    "        entities = rerank[q]\n",
    "        for ent in entities.keys():\n",
    "            printstring = str(int(entities[ent]['rel'])) + ' qid:' + q + \" 1:\" + str(entities[ent]['score']) + \" 2:\" + str(entities[ent]['fsdm_score']) + \" # \" + ent\n",
    "            print(printstring, file = f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef99ce-ddfe-4833-910a-f6188ee9b519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Loading auxilary files\")\n",
    "# Loading the file to rerank\n",
    "# Note, if an error is given here when loading a different file to rerank, try changing the seperator to '\\t'\n",
    "\n",
    "qrels_path = path_to_dbpedia + '/collection/v2/qrels-v2.txt'\n",
    "qrels = {}\n",
    "with open(qrels_path) as f:\n",
    "    for line in f:\n",
    "        terms = line.split()\n",
    "        if terms[0] not in qrels:\n",
    "            qrels[terms[0]] = {}\n",
    "        qrels[terms[0]][terms[2]] = terms[3]\n",
    "\n",
    "\n",
    "\n",
    "rerank_path = args.dbpedia_input\n",
    "#rerank =  pd.read_csv(rerank_path, sep='\\s+', names = ['query_id', 'x1', 'tag', 'rang', 'fsdm_score', 'x2'])\n",
    "rerank = {}\n",
    "with open(rerank_path) as f:\n",
    "    for line in f:\n",
    "        terms = line.split()\n",
    "        if terms[0] not in rerank:\n",
    "            rerank[terms[0]] = {}\n",
    "        if terms[2] in qrels[terms[0]]:\n",
    "            rel = qrels[terms[0]][terms[2]]\n",
    "        else:\n",
    "            rel = 0\n",
    "        rerank[terms[0]][terms[2]] = {'fsdm_score' : terms[4], 'rel' : rel}\n",
    "\n",
    "\n",
    "# Loading linked entities\n",
    "#confidence = pd.read_csv(args.linker)\n",
    "\n",
    "\n",
    "confidence = {}\n",
    "with open(args.linker) as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        terms = line.strip().split(',')\n",
    "        if terms[1] not in confidence:\n",
    "            confidence[terms[1]] = []\n",
    "        try: \n",
    "            confidence[terms[1]].append({'tag' : terms[2], 'confidence' : float(terms[3])})\n",
    "        except ValueError as e:\n",
    "            conf = terms[-1]\n",
    "            tag = ','.join(terms[2:-1])\n",
    "            confidence[terms[1]].append({'tag' : tag, 'confidence' : float(conf)})\n",
    "\n",
    "\n",
    "# Loading auxilary files\n",
    "#qrels_path = path_to_dbpedia + '/collection/v2/qrels-v2.txt'\n",
    "#qrels = pd.read_csv(qrels_path, sep='\\t',names = ['query_id', '', 'tag', 'rel'])\n",
    "queries_path = path_to_dbpedia + '/collection/v2/queries-v2.txt'\n",
    "queries = pd.read_csv(queries_path, sep='\\t',names = ['query_id', 'query'])\n",
    "\n",
    "# Loading previously computed redirects\n",
    "#df = pd.read_csv('/store/usr/gerritse/results_dict/wikipedia_redirect.csv')\n",
    "df = pd.read_csv('../Data/wikipedia_redirect.csv')\n",
    "\n",
    "# Getting the data ready for further processing in RankLib\n",
    "folds_path = path_to_dbpedia + \"/collection/v2/folds/all_queries.json\"\n",
    "with open(folds_path, 'r') as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6999ee-d68d-40f6-ad80-3745d3fb8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading embeddings\")\n",
    "# Loading the model with a Gensim keyedvector\n",
    "if args.embedding_type == 'rdf2vec' or args.embedding_type ==  'wikipedia2vec_old': \n",
    "    model = gensim.models.KeyedVectors.load(args.embedding, mmap='r')\n",
    "    \n",
    "elif args.embedding_type == 'wikipedia2vec':\n",
    "    from wikipedia2vec import Wikipedia2Vec\n",
    "    model = Wikipedia2Vec.load(args.embedding)\n",
    "\n",
    "elif args.embedding_type == 'complex': \n",
    "    import torch\n",
    "    from kge import Dataset, Config\n",
    "    from kge.model import KgeModel\n",
    "    from kge.util.io import load_checkpoint\n",
    "    checkpoint = load_checkpoint(args.embedding)\n",
    "    if args.ranklib_output == 'complex2vecpagelinksred':\n",
    "        dataset = dataset.create(checkpoint['config'],folder = '../src/dbpedia_kge_pagelinks')\n",
    "    else:\n",
    "        dataset = dataset.create(checkpoint['config'],folder = '../src/dbpedia_kge_small')\n",
    "\n",
    "    model = KgeModel.create_from(checkpoint, dataset)\n",
    "    entity_ids = model.dataset.entity_ids()\n",
    "    entity_ids_dict = {}\n",
    "    for i, e in enumerate(entity_ids):\n",
    "        entity_ids_dict[e] = i\n",
    "\n",
    "    import torch\n",
    "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "redirect_dict = {}\n",
    "for index, tags in df.iterrows():\n",
    "    redirect_dict[tags['original']] = tags['redirect']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff988f2-eca5-4829-af52-fda6d3add474",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity check:, if this gives an error, something went wrong\n",
    "entity_lookup(entity_converter('<dbpedia:Amsterdam>'), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49271410-6a17-437c-b9aa-51af6273ee28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scoring everything (might take a while)\n",
    "test_x = []\n",
    "\n",
    "f = open(args.score_output, 'w')\n",
    "\n",
    "print(\"Ranking entities:\")\n",
    "for query_id in rerank.keys():\n",
    "    print(query_id)\n",
    "    for tag in rerank[query_id].keys():        \n",
    "        query_based_score = ranking_feature(query_id, tag, conf = True)\n",
    "        test_x.append(query_based_score)\n",
    "        rerank[query_id][tag]['score'] = query_based_score\n",
    "        print(\" \".join([query_id, tag, str(query_based_score)] ), file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9965e3-ab3e-4895-b045-d653c2d7af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Saving files for Ranklib:\")\n",
    "for i in range(5):\n",
    "    query_fold = data[str(i)]\n",
    "    folder = args.ranklib_output + \"/Fold\" +str(i+1)\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    testpath = folder + \"/test.txt\"\n",
    "    print(\"Now writing test for fold \" + str(i+1))\n",
    "    to_print_format_dict(query_fold['testing'], testpath)\n",
    "\n",
    "    trainpath = folder + \"/train.txt\"\n",
    "    print(\"Now writing train for fold \" + str(i+1))\n",
    "    test_print = to_print_format_dict(query_fold['training'], trainpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a3066-40cb-4807-8327-a0eb66c94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ranklib_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885262c-6785-4d72-9947-4f035a35ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"bash Code/train_ranklib.sh {args.ranklib_output}\")\n",
    "print(f\"bash Code/score_ranklib.sh {args.ranklib_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f34ae-b706-4500-8db7-d6313590d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"sh eval_to_tex.sh ../{args.ranklib_output}/embed.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba271e1e-a56c-406b-8851-471fc4d70ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1b4a8-4f69-41b4-82ae-17b14f62c10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7ceae-48c6-43c7-bb8c-deca2e56202d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
